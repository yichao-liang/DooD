Attend-infer-repeat:
The idea behind the Attend-infer-repeat model is to decompose an image in different objects that can be
processed successively. Each detected object is described in a latent space by variables aiming to
represent what the object is (the class it belongs to), and where it is in the image
(its position as well as the correct spatial transformation).
These variables are then decoded to come back to the original space, generating an image that can be
compared to the input one.

For a given image, the first step consist in finding an region of interest where the model will “attend”
to, that is where the first object is located. To do that, the model guess if there is still an
unprocessed object (value of z_pres) in the image and if yes where it is (z_where).

When the image is restricted to the object of interest, the model will represent it in a latent space
using a restricted number of variables to compress the information (z_what). Using the decoder, these
variables will be transformed back to an image that would ideally be identical to the area of interest
provided in the first step. Then, according the value of z_pres, the generated region of interest
is added or not to the final image and z_where indicate the position as well as the spatial
transformation (e.g. scaling) needed.

If z_pres value were zero, there is no more detected object and the processing of the image is finished.
Otherwise, there is still an object unattended and we repeat the two previous steps. It is worth noting
that the results of the previous step (z_pres, z_what and z_where) are accessible for the next one,
which prevents to process the same object twice.

At the end, we can compute the distance between the original and generated images and tune the
parameters with gradient descent. We can cut the model in half: the inference part that use the image
to output the latent representation of each object (z_pres, z_what and z_where), and the generative
part that produce an image with each object given their representation in the latent space. These are
the two parts that need to be trained.

The attend-infer-repeat model, with the inference half, provides us an interesting representation
of an image with a high-level of interpretability. Each object is indeed represented on his own,
with distinct encoding of what the object is, and where it is located in the image.

Regarding the chosen model, the way the strokes are described iteratively using a RNN is similar. Each
stroke is processed one by one and transformed using an interpretable representation. However, the
“attend” part which consists in finding the regions of interest is done with a CNN and so all in once
and not iteratively.
